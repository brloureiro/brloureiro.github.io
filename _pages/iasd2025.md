---
layout: page
permalink: /teaching/iasd2025
title: Mathematics of deep learning
description:
nav: false
nav_order: 5
---

Welcome to the course page for the **Mathematics of deep learning** course at the *Intelligence Artificielle, Systèmes, Données - Informatique* ([IASD](https://dauphine.psl.eu/formations/masters/informatique/m2-intelligence-artificielle-systemes-donnees)) and the *Artificial Intelligence, Systems, Data - Mathematics Track* ([MASH](https://dauphine.psl.eu/en/training/masters-degrees/mathematics-and-applied-mathematics/masters-year-2-artificial-intelligence-systems-data/program)) M2 programs from [PSL University](https://psl.eu/). Here you'll find all the course information and materials.

## Course Information

- **Instructor**: Dr. Bruno Loureiro
- **Email**: [bruno.loureiro@di.ens.fr](mailto:bruno.loureiro@di.ens.fr)
- **Semester**: Winter 2025
- **Class Times**: Fridays, 9:00 AM - 12:15 AM
- **Location**: Check at the ENT.

## Evaluation

50% Homework + 50% paper presentation and discussion.

## Course Description

In the absence of a well-defined body of mathematics that could be called a bona fide "*theory of deep learning*", our goal in these lectures will be instead to introduce the students to some recent mathematical ideas that emerged in the study of deep learning. We don't aim at being exhaustive, but rather to train the students at reading paper and preparing them to do research in deep learning theory, inasmuch as this can be defined.

- Introduction and challenges.
- Universal approximation theorems
- The lazy limit of large-width networks.
- The double descent phenomena and benign overfitting.
- Implicit bias of GD/SGD.
- The speciation transition in diffusion models.

## Requirement

Undergraduate level linear algebra, analysis and probability will be assumed. Please check you are familiar with the [maths checklist](../assets/iasd/maths.pdf).

## Recommended literature

We will cover a variety of research topics which cannot be found in a single textbook. Nevertheless, when preparing this course I took inspiration in some excellent ressources which are freely available, and which you might also find useful:

- Francis Bach book [*Learning theory from first principles*](https://www.di.ens.fr/~fbach/ltfp_book.pdf).
- Matus Telgarsky *Deep learning theory* [lecture notes](https://mjt.cs.illinois.edu/dlt/two.pdf).
- Romain Couillet and Zhenyu Liao [book](https://polaris.imag.fr/romain.couillet/docs/RMT_ML_Book.pdf) on *Random Matrix Methods for Machine Learning*.
- Theodor Misiakiewicz and Andrea Montanari [*Six Lectures on Linearized Neural Networks*](https://arxiv.org/abs/2308.13431).

## Course Schedule

| Date        | Lecture Topic                | Materials                            |
|-------------|------------------------------|--------------------------------------|
| Jan 10     | - Motivation <br>  - Review of ERM         | [Notes](../assets/iasd/introduction.pdf) <br> [Homework 1](../assets/iasd/mathsdl_hw1.pdf)|
| Jan 17     | - Curse of dimensionality <br>  - Approximation theory | Sec. 1 to 3.1 of [Notes](../assets/iasd/approximation.pdf) <br> [Homework 2](../assets/iasd/mathsdl_hw2.pdf)|
| Jan 24     | - Approximation theory (continued) | Sec. 3.2 to 4 of [Notes](../assets/iasd/approximation.pdf) <br> [Homework 3](../assets/iasd/mathsdl_hw3.pdf)|
| Jan 31     | No class  |  |
| Feb 07     | - NTK | [Notes](../assets/iasd/ntk.pdf) <br> [Homework 4](../assets/iasd/mathsdl_hw4.pdf) |
| Feb 14     |  |  |
| Feb 21     |  |  |
| Feb 28     |  |  |
