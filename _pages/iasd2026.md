---
layout: page
permalink: /teaching/iasd2026
title: Mathematics of deep learning
description:
nav: false
nav_order: 5
---

Welcome to the course page for the **Mathematics of deep learning** course at the *Intelligence Artificielle, Systèmes, Données - Informatique* ([IASD](https://dauphine.psl.eu/formations/masters/informatique/m2-intelligence-artificielle-systemes-donnees)) M2 programs from [PSL University](https://psl.eu/). Here you'll find all the course information and materials.

## Course Information

- **Instructor**: Dr. Bruno Loureiro
- **Email**: [bruno.loureiro@di.ens.fr](mailto:bruno.loureiro@di.ens.fr)
- **Semester**: Winter 2026
- **Class Times**: Fridays, 9:00 AM - 12:15 AM
- **Location**: Check at the ENT.

## Evaluation

50% Homework + 50% paper presentation and discussion.

## Course Description

In the absence of a well-defined body of mathematics that could be called a bona fide "*theory of deep learning*", our goal in these lectures will be instead to introduce the students to some recent mathematical ideas that emerged in the study of deep learning. We don't aim at being exhaustive, but rather to train the students at reading paper and preparing them to do research in deep learning theory, inasmuch as this can be defined.

- Introduction and challenges.
- Universal approximation theorems
- The lazy limit of large-width networks.
- The double descent phenomena and benign overfitting.
- Implicit bias of GD/SGD.

## Requirement

Undergraduate level linear algebra, analysis and probability will be assumed. Please check you are familiar with the [maths checklist](../assets/iasd/maths.pdf).

## Recommended literature

The course is based on the following [lecture notes](../assets/iasd/lecture_notes.pdf).

When preparing them, I took inspiration in some excellent ressources which are freely available, and which you might also find useful:

- Francis Bach book [*Learning theory from first principles*](https://www.di.ens.fr/~fbach/ltfp_book.pdf).
- Matus Telgarsky *Deep learning theory* [lecture notes](https://mjt.cs.illinois.edu/dlt/two.pdf).
- Romain Couillet and Zhenyu Liao [book](https://polaris.imag.fr/romain.couillet/docs/RMT_ML_Book.pdf) on *Random Matrix Methods for Machine Learning*.
- Theodor Misiakiewicz and Andrea Montanari [*Six Lectures on Linearized Neural Networks*](https://arxiv.org/abs/2308.13431).
- Theodor Misiakiewicz lecture notes [*A Short Tutorial on the Computational Complexity of Deep Learning*](https://misiakie.github.io/publications/DRAFT_5_Lectures_Misiakiewicz.pdf)".
- Scott Pesme [PhD thesis](https://scottpesme.github.io/Articles/Scott_PhD_Manuscript.pdf) on linear diagonal neural networks.

## Course Schedule

| Date        | Lecture Topic                | Materials                            |
|-------------|------------------------------|--------------------------------------|
| Jan 16     | - Motivation <br>  - Review of ERM         | Chapter 1 of the [notes](../assets/iasd/lecture_notes.pdf) <br> [Homework 1](../assets/iasd/hw/mathsdl_hw1.pdf)|
| Jan 23     | - Curse of dimensionality <br>  - Aproximation theory | Chapter 2 & 3 of the [notes](../assets/iasd/lecture_notes.pdf) <br> [Homework 2](../assets/iasd/hw/mathsdl_hw2.pdf) |
| Jan 30     | - Aproximation theory (continued)| Chapter 3 of the [notes](../assets/iasd/lecture_notes.pdf) | [Homework 3](../assets/iasd/hw/mathsdl_hw3.pdf) |
| Feb 06     | No class  |  |
| Feb 07     | | |
| Feb 13     | | |
| Feb 20     | | |
| Feb 27     | | | 
| Mar 6      | | |
