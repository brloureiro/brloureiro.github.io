<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> news | Bruno Loureiro </title> <meta name="author" content="Bruno Loureiro"> <meta name="description" content="Personal webpage of Bruno Loureiro, researcher working on the crossroads between statistical physics and machine learning. "> <meta name="keywords" content="Machine Learning, Statistical Physics, high-dimensional Statistics"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%8D&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://brloureiro.github.io/news/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Bruno Loureiro </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/bio/">bio </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/group/">group </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">news</h1> <p class="post-description"></p> </header> <article> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Nov 03, 2024</th> <td> Our paper <a href="https://arxiv.org/abs/2305.18270" rel="external nofollow noopener" target="_blank"><em>How two-layer neural networks learn, one (giant) step at a time</em></a> was accepted for publication at JMLR! </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 26, 2024</th> <td> Our work <a href="https://arxiv.org/abs/2405.15699" rel="external nofollow noopener" target="_blank"><em>Dimension-free deterministic equivalents for random feature regression</em></a> has been accepted as a spotlight at <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS 2024</a>. Come see our poster in Vancouver! </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 13, 2024</th> <td> I will be teaching a statistical physics of learning course at 15th edition of the <a href="https://www.icts.res.in/program/bssp2024" rel="external nofollow noopener" target="_blank">Bangalore School on Statistical Physics</a>, a traditional event organised by the International Centre for Theoretical Sciences (ICTS). Looking forward! </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 01, 2024</th> <td> Excited to teach a course on statistical physics methods for machine learning theory <a href="https://mlschool.princeton.edu/" rel="external nofollow noopener" target="_blank">Princeton Machine Learning Theory Summer School</a> this summer. I have prepared some <a href="../assets/pdf/NotesPrinceton_BL.pdf">lecture notes</a>, check them out if you are interested! </td> </tr> <tr> <th scope="row" style="width: 20%">May 02, 2024</th> <td> Our three works: <ul> <li><a href="https://arxiv.org/abs/2402.04980" rel="external nofollow noopener" target="_blank"><em>Asymptotics of feature learning in two-layer networks after one gradient-step</em></a></li> <li><a href="https://arxiv.org/abs/2402.13999" rel="external nofollow noopener" target="_blank"><em>Asymptotics of Learning with Deep Structured (Random) Features</em></a></li> <li><a href="https://arxiv.org/abs/2406.02157" rel="external nofollow noopener" target="_blank"><em>Online Learning and Information Exponents: The Importance of Batch size &amp; Time/Complexity Tradeoffs</em></a></li> </ul> were accepted for publication at ICML 2024! See you in Vienna in July! </td> </tr> <tr> <th scope="row" style="width: 20%">May 01, 2024</th> <td> Together with my colleagues Vittorio Erba (EPFL) and Florent Krzakala (EPFL), I am organising the <em>Lausanne event on machine learning &amp; neural network theory</em> (<a href="https://lemanth2024.github.io/" rel="external nofollow noopener" target="_blank">LemanTh</a>) from May 27-29, 2024. Join in the beautiful city of Lausanne to discuss the recent progress in machine learning theory with an amazing lineup of speakers. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 26, 2024</th> <td> Our work <a href="https://arxiv.org/abs/2402.13622" rel="external nofollow noopener" target="_blank"><em>Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression</em></a> was accepted for publication at UAI 2024! Come check it out in Barcelona in July! </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 22, 2023</th> <td> Our paper <a href="https://arxiv.org/abs/2302.08933" rel="external nofollow noopener" target="_blank"><em>Universality laws for Gaussian mixtures in generalized linear modelss</em></a> in collaboration with Yatin Dandi, Ludovic Stephan, Florent Krzakala and Lenka Zdeborov√° has been accepted for a poster at the <a href="https://nips.cc/" rel="external nofollow noopener" target="_blank">Conference on Neural Information Processing Systems (NeurIPS)</a>. I will be also presenting our recent work <a href="https://arxiv.org/abs/2305.18502" rel="external nofollow noopener" target="_blank"><em>Escaping mediocrity: how two-layer networks learn hard generalized linear models</em></a> as an oral contribution to the <a href="https://opt-ml.org/" rel="external nofollow noopener" target="_blank">Optimization for Machine Learning</a> workshop. Come discuss with us in New Orleans! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 24, 2023</th> <td> Our papers: <ul> <li> <a href="https://arxiv.org/abs/2302.00401" rel="external nofollow noopener" target="_blank"><em>Deterministic equivalent and error universality of deep random features learning</em></a> in collaboration with Dominik Schroder, Hugo Cui and Daniil Dmitriev;</li> <li> <a href="https://arxiv.org/abs/2302.08923" rel="external nofollow noopener" target="_blank"><em>Are Gaussian Data All You Need? The Extents and Limits of Universality in High-Dimensional Generalized Linear Estimation</em></a> in collaboration with Luca Pesce, Florent Krzakala and Ludovic Stephan;</li> </ul> Have been accepted for a poster at the <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">International Conference on Machine Learning (ICML)</a>. Come discuss with us in Honolulu! </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 14, 2022</th> <td> Our two works <ul> <li><a href="https://arxiv.org/abs/2205.13527" rel="external nofollow noopener" target="_blank"><em>Subspace clustering in high-dimensions: Phase transitions &amp; Statistical-to-Computational gap</em></a></li> <li><a href="https://arxiv.org/abs/2202.00293" rel="external nofollow noopener" target="_blank"><em>Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks</em></a></li> </ul> were accepted for publication at NeurIPS 2022! See you virtually in New Orleans in December! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 15, 2022</th> <td> Our paper on the <a href="https://arxiv.org/abs/2201.13383" rel="external nofollow noopener" target="_blank"><em>Fluctuations, Bias, Variance &amp; Ensemble of Learners: Exact Asymptotics for Convex Losses in High-Dimension</em></a> has been accepted for a poster at the <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">International Conference on Machine Learning (ICML)</a>. Stay tuned and check our video out! </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 29, 2021</th> <td> Our three papers <ul> <li><a href="https://arxiv.org/abs/2106.03791" rel="external nofollow noopener" target="_blank">Learning Gaussian Mixtures with Generalised Linear Models: Precise Asymptotics in High-dimensions</a></li> <li><a href="https://arxiv.org/abs/2105.15004" rel="external nofollow noopener" target="_blank">Generalization Error Rates in Kernel Regression: The Crossover from the Noiseless to Noisy Regime</a></li> <li><a href="https://arxiv.org/abs/2102.08127" rel="external nofollow noopener" target="_blank">Learning curves of generic features maps for realistic datasets with a teacher-student model</a></li> </ul> were accepted for publication at NeurIPS 2021! See you virtually in December! </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 08, 2021</th> <td> Our new paper <a href="https://arxiv.org/abs/2106.03791" rel="external nofollow noopener" target="_blank"><em>Learning Gaussian Mixtures with Generalised Linear Models: Precise Asymptotics in High-dimensions</em></a> is out on arXiv! Check it out! </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 01, 2021</th> <td> Our new paper <a href="https://arxiv.org/abs/2105.15004" rel="external nofollow noopener" target="_blank"><em>Generalization Error Rates in Kernel Regression: The Crossover from the Noiseless to Noisy Regime</em></a> is out on arXiv! Check it out! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 06, 2021</th> <td> Our paper on the <a href="https://arxiv.org/abs/2006.14709" rel="external nofollow noopener" target="_blank"><em>The Gaussian equivalence of generative models for learning with shallow neural networks</em></a> has been accepted at the <a href="https://msml21.github.io/" rel="external nofollow noopener" target="_blank"><em>Mathematical and Scientific Machine Learning Conference (MSML2021)</em></a>, which due to the covid-19 is going to be online! Registration is open and free! </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 26, 2020</th> <td> Our work on the <a href="https://arxiv.org/abs/2006.05228" rel="external nofollow noopener" target="_blank">Phase retrieval in high dimensions: Statistical and computational phase transitions</a> has been accepted for <a href="https://nips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS 2020</a>! See you virtually in December! </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 09, 2020</th> <td> Our work on the <a href="https://arxiv.org/abs/2006.05228" rel="external nofollow noopener" target="_blank">Phase retrieval in high dimensions: Statistical and computational phase transitions</a> is out on arXiv! </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 01, 2020</th> <td> Our paper on the <a href="https://arxiv.org/abs/2002.09339" rel="external nofollow noopener" target="_blank"><em>Generalisation error in learning with random features and the hidden manifold model</em></a> has been accepted at the <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">International Conference on Machine Learning (ICML)</a>, which this year will be online, between the 12th and the 18th July. Stay tuned and check our video out. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 11, 2020</th> <td> Our paper on the <a href="https://arxiv.org/abs/1912.02008" rel="external nofollow noopener" target="_blank"><em>Exact asymptotics for phase retrieval and compressed sensing with random generative priors</em></a> has been accepted at the <a href="http://www.smartchair.org/hp/MSML2020" rel="external nofollow noopener" target="_blank"><em>Mathematical and Scientific Machine Learning Conference (MSML2020)</em></a>, which due to the covid-19 is going to be online! Registration is open and free! </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 19, 2019</th> <td> An extended version of the work we presented at the <a href="https://deep-inverse.org/" rel="external nofollow noopener" target="_blank"><em>Deep Inverse NeurIPS workshop</em></a> is now out on arXiv. <a href="https://arxiv.org/abs/1912.02008" rel="external nofollow noopener" target="_blank"><em>Check it out!</em></a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 05, 2019</th> <td> I will be presenting our ongoing work on the algorithmic and statistical thresholds for phase retrieval and compressed sensing in the NeurIPS 2019 workshop <a href="https://deep-inverse.org/" rel="external nofollow noopener" target="_blank"><em>Solving inverse problems with deep networks: New architectures, theoretical foundations, and applications</em></a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 04, 2019</th> <td> Our paper <a href="https://arxiv.org/abs/1905.12385" rel="external nofollow noopener" target="_blank"><em>‚ÄúThe spiked matrix model with generative priors‚Äù</em></a> was accepted for publication at NeurIPS 2019. See you in Vancouver! </td> </tr> <tr> <th scope="row" style="width: 20%">May 29, 2019</th> <td> Our new paper <em>‚ÄúThe spiked matrix model with generative priors‚Äù</em> is out! Check it on the <a href="https://arxiv.org/abs/1905.12385" rel="external nofollow noopener" target="_blank">arXiv</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 11, 2024</th> <td> <a class="news-title" href="/teaching/iaso2024">Statistical Learning II</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 01, 2024</th> <td> In our <a href="https://arxiv.org/abs/2410.18938" rel="external nofollow noopener" target="_blank">recent pre-print</a>, we provide a rigorous random matrix theory analysis of how feature learning impacts generalisation in two-layer neural networks after a single aggressive gradient step. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 21, 2024</th> <td> Regularisation plays an important role in hedging against adversarial attacks. In our <a href="https://arxiv.org/pdf/2410.16073" rel="external nofollow noopener" target="_blank">recent pre-print</a> we show that regularising with respect to the dual norm of the attacker is often a good strategy, but that it can be suboptimal when data is scarce. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 18, 2024</th> <td> Variational inference (VI) is the problem of approximating a complicated distribution with a distribution which is simpler to sample. A major hinder for VI is mode collapse: the phenomenon where the model concentrates on a few modes of the target distribution during training, despite being expressive enough to cover all modes. This dynamical phenomenon is poorly understood theoretically. In our <a href="https://arxiv.org/pdf/2410.13300" rel="external nofollow noopener" target="_blank">recent pre-print</a> we identify different mechanisms driving mode collapse in a Gaussian mixture model setting, showing that they share some similarities with what is found in normalising flows. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 04, 2024</th> <td> <a href="https://arxiv.org/abs/2406.02157" rel="external nofollow noopener" target="_blank">Online Learning and Information Exponents: On The Importance of Batch size, and Time/Complexity Tradeoffs</a> is finally out on arXiv. Check it out! </td> </tr> <tr> <th scope="row" style="width: 20%">May 24, 2024</th> <td> Two exiting projects I have been working for the past year are finally out on arXiv. Check them out! <ul> <li><a href="https://arxiv.org/abs/2405.15699" rel="external nofollow noopener" target="_blank">Dimension-free deterministic equivalents for random feature regression</a></li> <li><a href="https://arxiv.org/abs/2405.15480" rel="external nofollow noopener" target="_blank">Fundamental limits of weak learnability in high-dimensional multi-index models</a></li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 21, 2024</th> <td> Gaussian ensembles of deep random neural networks have recently made a come back as it was shown they can, in some situations, capture the generalisation behaviour of trained networks. In our <a href="https://arxiv.org/abs/2402.13999" rel="external nofollow noopener" target="_blank">recent pre-print</a>, we provide an exact asymptotic analysis of the training of the last layer of such ensembles. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 21, 2024</th> <td> Resampling techniques such as bagging, the jackknife and the bootstrap are important tools to compute uncertainty of estimators in classical statistics. In <a href="https://arxiv.org/abs/2402.13622" rel="external nofollow noopener" target="_blank">our recent pre-print</a> we answer the question: are they reliable in a high-dimensional regime? </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 08, 2024</th> <td> Neural networks are notably susceptible to adversarial attacks. Understanding which features in the training data are more susceptible and how to protect them is therefore an important endeavour. In <a href="https://arxiv.org/abs/2402.05674" rel="external nofollow noopener" target="_blank">our recent pre-print</a> we introduce a synthetic model of structured data which captures this phenomenology, and provide an exact asymptotic solution of adversarial training in this model. In particular, we identify a generalisation vs. robustness trade-off, and propose some strategies to defend non-robust features. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 07, 2024</th> <td> Understanding how neural-networks learn features during training and how these impact their capacity to generalise is an important open question. In <a href="https://arxiv.org/abs/2402.04980" rel="external nofollow noopener" target="_blank">our recent pre-print</a>, we provide a sharp analysis of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 28, 2023</th> <td> Most of the asymptotic analysis in the proportional regime rely on Gaussianity on the covariates. In our new work <a href="https://arxiv.org/abs/2309.16476" rel="external nofollow noopener" target="_blank"><em>High-dimensional robust regression under heavy-tailed data: Asymptotics and Universality</em></a> with Urte Adomaityte, Leonardo Defilippis and Gabriele Sicuro we provide an asymptotic analysis for generalised linear models trained on heavy-tailed covariates. In particular, we investigate the impact of heavy-tailed contamination to robust M-estimators! Check it out! </td> </tr> <tr> <th scope="row" style="width: 20%">May 14, 2023</th> <td> Our paper <a href="https://arxiv.org/abs/2302.05882" rel="external nofollow noopener" target="_blank"><em>From high-dimensional &amp; mean-field dynamics to dimensionless ODEs: A unifying approach to SGD in two-layers networks</em></a> in collaboration with Luca Arnaboldi, Florent Krzakala and Ludovic Stephan has been accepted for a poster at the <a href="https://learningtheory.org/colt2023/" rel="external nofollow noopener" target="_blank">Conference on Learning Theory (COLT)</a>. Come discuss with us in Bangalore! </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 17, 2023</th> <td> Two recent works on universality results for generalised linear models are now out: <ul> <li><a href="https://arxiv.org/abs/2302.08933" rel="external nofollow noopener" target="_blank"><em>Universality laws for Gaussian mixtures in generalized linear models</em></a></li> <li><a href="https://arxiv.org/abs/2302.08923" rel="external nofollow noopener" target="_blank"><em>Are Gaussian data all you need? Extents and limits of universality in high-dimensional generalized linear estimation</em></a></li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 02, 2023</th> <td> Our new paper titled <a href="https://arxiv.org/abs/2302.05882" rel="external nofollow noopener" target="_blank"><em>From high-dimensional &amp; mean-field dynamics to dimensionless ODEs: A unifying approach to SGD in two-layers networks</em></a> is now out! In this work, we derive different limits for the one-pass SGD dynamics of two-layer neural networks, including: a) the classical gradient flow limit; b) the high-dimensional limit studied by <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.74.4337" rel="external nofollow noopener" target="_blank">Savid Saad &amp; Sara Solla in the 90s</a>; c) and the more recent infinite width mean-field limit. Check it out! </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 01, 2023</th> <td> <a href="https://arxiv.org/abs/2302.00401" rel="external nofollow noopener" target="_blank"><em>Deterministic equivalent and error universality of deep random features learning</em></a>, work done in collaboration with Dominik Schr√∂der, Hugo Cui and Daniil Dmitriev is now out on arXiv. In this work, we prove a deterministic equivalent for the resolvent of deep random features sample covariance matrices, which allow us to establish Gaussian universality of the test error in ridge regression. We also conjecture (and provide extensive support) for the error universality for other loss functions, allowing us to derive an asymptotic formula for the asymptotic performance beyond ridge regression. Check it out! </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 19, 2023</th> <td> Our paper on the <a href="https://arxiv.org/abs/2210.12760" rel="external nofollow noopener" target="_blank"><em>A study of uncertainty quantification in overparametrized high-dimensional models</em></a> has been accepted at the <a href="http://aistats.org/aistats2023/" rel="external nofollow noopener" target="_blank">International Conference on Artificial Intelligence and Statistics (AISTATS)</a>. See you in Valencia in April! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 26, 2022</th> <td> Gaussianity of the input data is a classic assumption in high-dimensional statistics. Although this might seen unrealistic at a first sight, due to strong universality properties it turns out that this assumption is not so stringent in high-dimensions. This is the subject of our recent work <a href="https://arxiv.org/abs/2205.13303" rel="external nofollow noopener" target="_blank"><em>Gaussian Universality of Linear Classifiers with Random Labels in High-Dimension</em></a>! Check it out! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 26, 2022</th> <td> A widespread answer for the ‚Äúcurse of dimensionality‚Äù in machine learning is that the relevant features in data are often low-dimensional embeddings in a higher dimensional space. Subspace clustering is precisely the unsupervised task of finding low-dimensional clusters in data. In our recent work <a href="https://arxiv.org/abs/2205.13527" rel="external nofollow noopener" target="_blank"><em>Subspace clustering in high-dimensions: Phase transitions &amp; Statistical-to-Computational gap</em></a> we characterise the statistical-to-computational trade-offs of subspace clustering in a simple Gaussian mixture model where the relevant features (the means) are sparse vectors. Check it out! </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 28, 2022</th> <td> One of the most classical results in high-dimensional learning theory provides a closed-form expression for the generalisation error of binary classification with the single-layer teacher-student perceptron on i.i.d. Gaussian inputs. Surprisingly, an analogous analysis for the corresponding multi-class teacher-student perceptron was missing. In our new work <a href="https://arxiv.org/abs/2203.12094" rel="external nofollow noopener" target="_blank"><em>Learning curves for the multi-class teacher-student perceptron</em></a> we fill this gap! Check it out! </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 09, 2022</th> <td> A couple of exiting projects I have been working for the past year are finally out on arXiv. Check them out! <ul> <li><a href="https://arxiv.org/abs/2202.03295" rel="external nofollow noopener" target="_blank">Theoretical characterization of uncertainty in high-dimensional linear classification</a></li> <li><a href="https://arxiv.org/abs/2202.00293" rel="external nofollow noopener" target="_blank">Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks</a></li> <li><a href="https://arxiv.org/abs/2201.13383" rel="external nofollow noopener" target="_blank">Fluctuations, Bias, Variance &amp; Ensemble of Learners: Exact Asymptotics for Convex Losses in High-Dimension</a></li> <li><a href="https://arxiv.org/abs/2201.12655" rel="external nofollow noopener" target="_blank">Error Rates for Kernel Classification under Source and Capacity Conditions</a></li> <li><a href="https://arxiv.org/abs/2201.09986" rel="external nofollow noopener" target="_blank">Bayesian Inference with Nonlinear Generative Models: Comments on Secure Learning</a></li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 17, 2021</th> <td> Our new paper <a href="https://arxiv.org/abs/2102.08127" rel="external nofollow noopener" target="_blank"><em>Capturing the learning curves of generic features maps for realistic data sets with a teacher-student model</em></a> is out on arXiv! Check it out! </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 14, 2020</th> <td> Our new paper on the <a href="https://arxiv.org/abs/2006.14709" rel="external nofollow noopener" target="_blank"><em>The Gaussian equivalence of generative models for learning with shallow neural networks</em></a> is out on arXiv! Check it out! </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 19, 2020</th> <td> We will be presenting our work on the <a href="https://arxiv.org/abs/2002.09339" rel="external nofollow noopener" target="_blank"><em>Generalisation error in learning with random features and the hidden manifold model</em></a> on <a href="https://deepmath-conference.com" rel="external nofollow noopener" target="_blank">DeepMath 2020</a>! Come check our poster! </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 21, 2020</th> <td> Our new paper on the <a href="https://arxiv.org/abs/2002.09339" rel="external nofollow noopener" target="_blank"><em>Generalisation error in learning with random features and the hidden manifold model</em></a> is out on arXiv! Check it out! </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 07, 2015</th> <td> <a class="news-title" href="/news/announcement_2/">A long announcement with details</a> </td> </tr> </table> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2024 Bruno Loureiro. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KENCKL5VM"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7KENCKL5VM");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>